{
  "conversation": [
    {
      "speaker": "user",
      "text_prompt": "NEWS | 27 May 2021 | Clarification 28 May 2021  Hundreds of gibberish papers still\nlurk in the scientific literature  The nonsensical computer-generated articles, spotted years after the problem was first seen, could lead to a wave of retractions.  Richard Van Noorden  Nonsensical research papers generated by a computer program are still popping up in the scientific literature many years after the problem was first seen, a study has revealed. Some publishers have told Nature they will take down the papers, which could result in more than 200 retractions. The issue began in 2005, when three PhD students created paper-generating software called SCIgen for *maximum amusement*, and to show that some conferences would accept meaningless papers. The program cobbles together words to generate research articles with random titles, text and charts, easily spotted as gibberish by a human reader. It is free to download, and anyone can use it. By 2012, computer scientist Cyril LabbÃ© had found 85 fake SCIgen papers in conferences published by the Institute of Electrical and Electronic Engineers (IEEE); he went on to find more than 120 fake SCIgen papers published by the IEEE and by Springer. It was unclear",
      "start": "",
      "end": "",
      "visual_prompt": [
        ["nature > news > article", 1.0]
      ],
      "mida": []
    },
    {
      "speaker": "user",
      "text_prompt": "Sometimes I just, I can't even believe this is a headline.",
      "start": "00:00:04",
      "end": "00:00:06",
      "visual_prompt": [],
      "mida": []
    },
    {
      "speaker": "user",
      "text_prompt": "Like, like, imagine the same thing at my job would be, like, if somebody on our team just, like, added a bunch of junk files and just, like, committed those and added those.",
      "start": "00:00:07",
      "end": "00:00:16",
      "visual_prompt": [],
      "mida": []
    },
    {
      "speaker": "user",
      "text_prompt": "It's like, we look at that stuff.",
      "start": "00:00:16",
      "end": "00:00:18",
      "visual_prompt": [],
      "mida": []
    },
    {
      "speaker": "user",
      "text_prompt": "We would not allow that to happen.",
      "start": "00:00:18",
      "end": "00:00:20",
      "visual_prompt": [],
      "mida": []
    },
    {
      "speaker": "user",
      "text_prompt": "But what's funny about science is they're, like...",
      "start": "00:00:21",
      "end": "00:00:22",
      "visual_prompt": [],
      "mida": []
    },
    {
      "speaker": "user",
      "text_prompt": "oh yeah, like, oh yeah, we'll peer review this shit.",
      "start": "00:00:23",
      "end": "00:00:27",
      "visual_prompt": [],
      "mida": []
    },
    {
      "speaker": "user",
      "text_prompt": "It's like these fucking gibberish papers are just floating around.",
      "start": "00:00:27",
      "end": "00:00:30",
      "visual_prompt": [],
      "mida": []
    },
    {
      "speaker": "user",
      "text_prompt": "People are probably getting paid and making money off of just literally absolutely nothing.",
      "start": "00:00:30",
      "end": "00:00:36",
      "visual_prompt": [],
      "mida": []
    },
    {
      "speaker": "assistant",
      "text_prompt": "literally if its hard to understand it must be legit accept so i can keep larping as smart",
      "start": "",
      "end": "",
      "visual_prompt": [],
      "mida": []
    },
    {
      "speaker": "assistant",
      "text_prompt": "easily spotted by a human reader but they still cant find them all n had to write more code to find them which doesnt even work rip",
      "start": "",
      "end": "",
      "visual_prompt": [],
      "mida": []
    }
  ]
}
